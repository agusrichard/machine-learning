{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essentials\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Visualizing a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(\n",
    "    tree_clf,\n",
    "    out_file='iris_tree.dot',\n",
    "    feature_names=iris.feature_names[2:],\n",
    "    class_names=iris.target_names,\n",
    "    rounded=True,\n",
    "    filled=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the many qualities of Decision Trees is that they require\n",
    "very little data preparation. In particular, they don’t require feature\n",
    "scaling or centering at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A node’s samples attribute counts how many training instances it applies to. For\n",
    "example, 100 training instances have a petal length greater than 2.45 cm (depth 1,\n",
    "right), among which 54 have a petal width smaller than 1.75 cm (depth 2, left). A\n",
    "node’s value attribute tells you how many training instances of each class this node\n",
    "applies to: for example, the bottom-right node applies to 0 Iris-Setosa, 1 Iris-\n",
    "Versicolor, and 45 Iris-Virginica. Finally, a node’s gini attribute measures its impurity:\n",
    "a node is “pure” (gini=0) if all training instances it applies to belong to the same\n",
    "class. For example, since the depth-1 left node applies only to Iris-Setosa training\n",
    "instances, it is pure and its gini score is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the depth-2 left node\n",
    "has a gini score equal to $1 – (0/54)^2 – (49/54)^2 – (5/54)^2 ≈ 0.168$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$G_i = 1 - \\sum_{k=1}^{n}p_{i,k}^2$\n",
    "\n",
    "$p_{i,k}$ is the ratio of class k instances among the training instances in the ith node.\n",
    "Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn uses the CART algorithms, which produces only binary trees: nonleaf nodes always have two children."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Class Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Decision Tree can also estimate the probability that an instance belongs to a particular\n",
    "class k: first it traverses the tree to find the leaf node for this instance, and then it\n",
    "returns the ratio of training instances of class k in this node. For example, suppose\n",
    "you have found a flower whose petals are 5 cm long and 1.5 cm wide. The corresponding\n",
    "leaf node is the depth-2 left node, so the Decision Tree should output the\n",
    "following probabilities: 0% for Iris-Setosa (0/54), 90.7% for Iris-Versicolor (49/54),\n",
    "and 9.3% for Iris-Virginica (5/54). And of course if you ask it to predict the class, it\n",
    "should output Iris-Versicolor (class 1) since it has the highest probability. Let’s check\n",
    "this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.90740741, 0.09259259]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CART Training Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn uses the Classification And Regression Tree (CART) algorithm to train\n",
    "Decision Trees (also called “growing” trees). The idea is really quite simple: the algorithm\n",
    "first splits the training set in two subsets using a single feature k and a threshold\n",
    "tk (e.g., “petal length ≤ 2.45 cm”). How does it choose k and tk? It searches for the\n",
    "pair (k, tk) that produces the purest subsets (weighted by their size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(k, t_k) = \\frac{m_{left}}{m}G_{left} + \\frac{m_{right}}{m}G_{right}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ where \\left\\{\n",
    "\\begin{array}{ll}\n",
    "    G_{left/right} & measures\\:the\\:impurity\\:of\\:the\\:left/right\\:subset, \\\\\n",
    "    m_{left/right} & is\\:the\\:number\\:of\\:instances\\:in\\:the\\:left/right\\:subset.\\\\\n",
    "\\end{array}\n",
    "\\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it has successfully split the training set in two, it splits the subsets using the\n",
    "same logic, then the sub-subsets and so on, recursively. It stops recursing once it reaches\n",
    "the maximum depth (defined by the max_depth hyperparameter), or if it cannot\n",
    "find a split that will reduce impurity. A few other hyperparameters control additional stopping conditions (min_samples_split, min_sam\n",
    "ples_leaf, min_weight_fraction_leaf, and max_leaf_nodes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART algorithm is greedy algorithmm: it greedily searches for an optimum split at the top level, then repeart the process at each level. It does not check whether or not the split will lead to the lowest possible impurity several levels down. A greedy algorithm often produces a reasonably good solution, but it is not guaranteed to be the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the optimal tree requires O(exp(m)) time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting complexity: $O(log_2(m))$ <br>\n",
    "Training complexity: $O(n\\:x\\:m\\,log(m))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gini Impurity or Entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the Gini impurity measure is used, but you can select the entropy impurity measure instead by setting the criterion hyperparameter to \"entropy\". The concept of entropy originated in thermodynamics as a measure of molecular disorder: entropy approaches zero when molecules are still and well ordered. It later spread to a wide variety of domains, including Shannon’s information theory, where it measures the average information content of a message: entropy is zero when all messages are identical. In Machine Learning, it is frequently used as an impurity measure: a set’sentropy is zero when it contains instances of only one class. For example, the depth-2 left node has an entropy equal to $−\\frac{49}{54}\\;log\\;(\\frac{49}{54})\\:−\\:\\frac{5}{54}\\;log\\;(\\frac{5}{54}) ≈ 0.31.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy score:\n",
    "\n",
    "$H_i = \\sum_{k=1}^{n}\\:p_{i, k}\\;log(p_{i, k})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So should you use Gini impurity or entropy? The truth is, most of the time it does not make a big difference: they lead to similar trees. Gini impurity is slightly faster to compute, so it is a good default. However, when they differ, Gini impurity tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce slightly more balanced trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees make very few assumptions about the training data (as opposed to linear\n",
    "models, which obviously assume that the data is linear, for example). If left\n",
    "unconstrained, the tree structure will adapt itself to the training data, fitting it very\n",
    "closely, and most likely overfitting it. Such a model is often called a nonparametric\n",
    "model, not because it does not have any parameters (it often has a lot) but because the\n",
    "number of parameters is not determined prior to training, so the model structure is\n",
    "free to stick closely to the data. In contrast, a parametric model such as a linear model\n",
    "has a predetermined number of parameters, so its degree of freedom is limited,\n",
    "reducing the risk of overfitting (but increasing the risk of underfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid overfitting the training data, you need to restrict the Decision Tree’s freedom\n",
    "during training. As you know by now, this is called regularization. The regularization\n",
    "hyperparameters depend on the algorithm used, but generally you can at least restrict\n",
    "the maximum depth of the Decision Tree. In Scikit-Learn, this is controlled by the\n",
    "max_depth hyperparameter (the default value is None, which means unlimited).\n",
    "Reducing max_depth will regularize the model and thus reduce the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DecisionTreeClassifier class has a few other parameters that similarly restrict the shape of the Decision Tree: min_samples_split (the minimum number of samples a node must have before it can be split), min_samples_leaf (the minimum number\n",
    "of samples a leaf node must have), min_weight_fraction_leaf (same as\n",
    "min_samples_leaf but expressed as a fraction of the total number of weighted\n",
    "instances), max_leaf_nodes (maximum number of leaf nodes), and max_features\n",
    "(maximum number of features that are evaluated for splitting at each node). Increasing\n",
    "min_* hyperparameters or reducing max_* hyperparameters will regularize the\n",
    "model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other algorithms work by first training the Decision Tree without restrictions, then pruning (deleting) unnecessary nodes. A node whose children are all leaf nodes is considered unnecessary if the purity improvement it provides is not statistically significant. Standard statistical tests, such as the χ2 test, are used to estimate the probability that the improvement is purely the result of chance (which is called the null hypothesis). If this probability, called the pvalue, is higher than a given threshold (typically 5%, controlled by a hyperparameter), then the node is considered unnecessary and its children are deleted. The pruning continues until all unnecessarynodes have been pruned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tree looks very similar to the classification tree you built earlier. The main difference\n",
    "is that instead of predicting a class in each node, it predicts a value. For example,\n",
    "suppose you want to make a prediction for a new instance with x1 = 0.6. You traverse\n",
    "the tree starting at the root, and you eventually reach the leaf node that predicts\n",
    "value=0.1106. This prediction is simply the average target value of the 110 training\n",
    "instances associated to this leaf node. This prediction results in a Mean Squared Error\n",
    "(MSE) equal to 0.0151 over these 110 instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CART algorithm works mostly the same way as earlier, except that instead of trying\n",
    "to split the training set in a way that minimizes impurity, it now tries to split the\n",
    "training set in a way that minimizes the MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART cost function for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(k, t_k) = \\frac{m_{left}}{m}\\,MSE_{left} + \\frac{m_{right}}{m}\\,MSE_{right}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$where \\left\\{\n",
    "\\begin{array}{ll}\n",
    "    MSE_{node} = \\sum_{i \\in node} (\\hat{y}_{node} - y^{(i)})^2, \\\\\n",
    "    \\hat{y}_node = \\frac{1}{m_{node}} \\sum_{i \\in node} y^{(i)}. \\\\\n",
    "\\end{array}\n",
    "\\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like for classification tasks, Decision Trees are prone to overfitting when dealing\n",
    "with regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully by now you are convinced that Decision Trees have a lot going for them:\n",
    "they are simple to understand and interpret, easy to use, versatile, and powerful.\n",
    "However they do have a few limitations. First, as you may have noticed, Decision\n",
    "Trees love orthogonal decision boundaries (all splits are perpendicular to an axis),\n",
    "which makes them sensitive to training set rotation. To solve the problem, we can use PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train and fine-tune a Decision Tree for the moons dataset.\n",
    "a. Generate a moons dataset using make_moons(n_samples=10000, noise=0.4). <br>\n",
    "b. Split it into a training set and a test set using train_test_split(). <br>\n",
    "c. Use grid search with cross-validation (with the help of the GridSearchCV class) to find good hyperparameter values for a DecisionTreeClassifier. Hint: try various values for max_leaf_nodes. <br>\n",
    "d. Train it on the full training set using these hyperparameters, and measure\n",
    "your model’s performance on the test set. You should get roughly 85% to 87% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=10000, noise=0.4)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_leaf_nodes' : list(range(2, 31))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid=param_grid, \n",
    "                           cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'max_leaf_nodes': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=19,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=19,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8452"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Grow a forest.\n",
    "\n",
    "a. Continuing the previous exercise, generate 1,000 subsets of the training set,\n",
    "each containing 100 instances selected randomly. Hint: you can use Scikit-\n",
    "Learn’s ShuffleSplit class for this. <br>\n",
    "b. Train one Decision Tree on each subset, using the best hyperparameter values\n",
    "found above. Evaluate these 1,000 Decision Trees on the test set. Since they\n",
    "were trained on smaller sets, these Decision Trees will likely perform worse\n",
    "than the first Decision Tree, achieving only about 80% accuracy. <br>\n",
    "c. Now comes the magic. For each test set instance, generate the predictions of\n",
    "the 1,000 Decision Trees, and keep only the most frequent prediction (you can\n",
    "use SciPy’s mode() function for this). This gives you majority-vote predictions\n",
    "over the test set. <br>\n",
    "d. Evaluate these predictions on the test set: you should obtain a slightly higher\n",
    "accuracy than your first model (about 0.5 to 1.5% higher). Congratulations,\n",
    "you have trained a Random Forest classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_X = np.empty(shape=(1000, 100, 2), dtype=float)\n",
    "array_y = np.empty(shape=(1000, 100), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    bootstrap = generator.choice(10000, 100, replace=True)\n",
    "    array_X[i] = X[bootstrap]\n",
    "    array_y[i] = y[bootstrap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.empty(shape=(1000))\n",
    "for i in range(1000):\n",
    "    grid_search.best_estimator_.fit(array_X[i], array_y[i])\n",
    "    scores[i] = grid_search.best_estimator_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7866484"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
