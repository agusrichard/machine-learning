{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essentials\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will learn two different ways to train the linear regression model:\n",
    "- Using a direct “closed-form” equation that directly computes the model parameters that best fit the model to the training set (i.e., the model parameters that minimize the cost function over the training set).\n",
    "- Using an iterative optimization approach, called Gradient Descent (GD), that gradually tweaks the model parameters to minimize the cost function over the training set, eventually converging to the same set of parameters as the first method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generally, a linear model makes a prediction by simply computing a weighted sum of the input features, plus a constant called the bias term (also called the intercept term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\hat{y} = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + ... + \\theta_{n}x_{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized form:\n",
    "\n",
    "$ \\hat{y} = h_{\\theta}(x) = \\theta^{T} x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a Linear Regression model, you need to find the value of θ that minimizes the RMSE. In practice, it is simpler to minimize the Mean Square Error (MSE) than the RMSE, and it leads to the same result (because the value that minimizes afunction also minimizes its square root).1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$MSE(X, h_{\\theta}) = \\frac{1}{m} \\sum_{i=1}^m (\\theta^{T} x^{(i)} - y^{(i)})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{\\theta} = (X^{T} . X)^-1 . X^T . y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((100, 1)), X] # add x0 = 1 to each instance\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.19464569],\n",
       "       [2.74353518]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.19464569],\n",
       "       [9.68171605]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instance\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_new, y_predict, \"r-\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.19464569]), array([[2.74353518]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.19464569],\n",
       "       [9.68171605]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computational complexity of inverting such a matrix is typically about O(n2.4) to O(n3) (depending on the implementation). In other words, if you double the number of features, you multiply the computation time by roughly 22.4 = 5.3 to 23 = 8. The normal equation gets very slow when the number of features grows large (e.g., 100,000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the positive side, this equation is linear with regards to the number of instances in\n",
    "the training set (it is O(m)), so it handles large training sets efficiently, provided they\n",
    "can fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is totweak parameters iteratively in order to minimize a cost function. Gradient Descent does: it measures the local gradient of the error function with regards to the parameter vector θ, and it goes in the direction of descending gradient. Once the gradient is zero, you have reached a minimum\n",
    "\n",
    "Concretely, you start by filling θ with random values (this is called random initialization), and then you improve it gradually, taking one baby step at a time, each step attempting to decrease the cost function (e.g., the MSE), until the algorithm converges to a minimum.\n",
    "\n",
    "An important parameter in Gradient Descent is the size of the steps, determined by the learning rate hyperparameter. If the learning rate is too small, then the algorithmwill have to go through many iterations to converge, which will take a long time.\n",
    "\n",
    "On the other hand, if the learning rate is too high, you might jump across the valley and end up on the other side, possibly even higher up than you were before. This might make the algorithm diverge, with larger and larger values, failing to find a good solution.\n",
    "\n",
    "Finally, not all cost functions look like nice regular bowls. There may be holes, ridges, plateaus, and all sorts of irregular terrains, making convergence to the minimum very difficult. The two main challenges with Gradient Descent: if the random initialization starts the algorithm on the left, then it will converge to a local minimum, which is not as good as the global minimum. If it starts on the right, then it will take a very long time to cross the plateau, and if you stop too early you will never reach the global minimum.\n",
    "\n",
    "Fortunately, the MSE cost function for a Linear Regression model happens to be a convex function, which means that if you pick any two points on the curve, the line segment joining them never crosses the curve. This implies that there are no local minima, just one global minimum. It is also a continuous function with a slope that never changes abruptly.4 These two facts have a great consequence: Gradient Descent is guaranteed to approach arbitrarily close the global minimum (if you wait long enough and if the learning rate is not too high).\n",
    "\n",
    "In fact, the cost function has the shape of a bowl, but it can be an elongated bowl if the features have very different scales. Notes: scaled data can improve the performance of gradient descent because the path to minimum will be more direct, than the not-scaled data.\n",
    "\n",
    "Training a model means searching for a combination of model parameters that minimizes a cost function (over the training set). It is a search in the model’s parameter space: the more parameters a model has, the more dimensions this space has, and the harder the search is: searching for a needle in a 300-dimensional haystack is much trickier than in three dimensions. Fortunately, since the cost function is convex in the case of Linear Regression, the needle is simply at the bottom of the bowl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial derivaties of the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial Q}{\\partial t} MSE(\\theta)= \\frac{1}{2m} \\sum_{i=1}^m (\\theta^{T} x^{(i)} - y^{(i)})x^{(i)}_{j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient vector of the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\nabla_{\\theta} MSE(θ) = \\frac{2}{m} X^T . (X.\\theta - y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this formula involves calculations over the full training\n",
    "set X, at each Gradient Descent step! This is why the algorithm is called Batch Gradient Descent: it uses the whole batch of training data at every step. As a result it is terribly slow on very large training sets (but we will see much faster Gradient Descent algorithms shortly). However, Gradient Descent scales well with the number of features; training a Linear Regression model when there are hundreds of thousands of features is much faster using Gradient Descent than using the Normal Equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent step:\n",
    "\n",
    "$\\theta^{(next step)} = \\theta - \\eta \\nabla_{\\theta} MSE(θ)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1 # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2, 1) # random initialization\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.19464569],\n",
       "       [2.74353518]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find a good learning rate, we can use grid search. However, you\n",
    "may want to limit the number of iterations so that grid search can eliminate models\n",
    "that take too long to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may wonder how to set the number of iterations. If it is too low, you will still be\n",
    "far away from the optimal solution when the algorithm stops, but if it is too high, you\n",
    "will waste time while the model parameters do not change anymore. A simple solution\n",
    "is to set a very large number of iterations but to interrupt the algorithm when the\n",
    "gradient vector becomes tiny—that is, when its norm becomes smaller than a tiny\n",
    "number ϵ (called the tolerance)—because this happens when Gradient Descent has\n",
    "(almost) reached the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence rate:\n",
    "\n",
    "When the cost function is convex and its slope does not change abruptly (as is the\n",
    "case for the MSE cost function), it can be shown that Batch Gradient Descent with a\n",
    "fixed learning rate has a convergence rate of $O(\\frac{1}{iterations})$\n",
    "iterations . In other words, if you divide\n",
    "the tolerance ϵ by 10 (to have a more precise solution), then the algorithm will have\n",
    "to run about 10 times more iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problem with Batch Gradient Descent is the fact that it uses the whole\n",
    "training set to compute the gradients at every step, which makes it very slow when\n",
    "the training set is large. At the opposite extreme, Stochastic Gradient Descent just\n",
    "picks a random instance in the training set at every step and computes the gradients\n",
    "based only on that single instance. Obviously this makes the algorithm much faster\n",
    "since it has very little data to manipulate at every iteration. It also makes it possible to\n",
    "train on huge training sets, since only one instance needs to be in memory at each\n",
    "iteration (SGD can be implemented as an out-of-core algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, due to its stochastic (i.e., random) nature, this algorithm is much less regular than Batch Gradient Descent: instead of gently decreasing until it reaches the minimum, the cost function will bounce up and down, decreasing only on average. Over time it will end up very close to the minimum, but once it gets there it will continue to bounce around, never settling down. So once the algorithm  stops, the final parameter values are good, but not optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the cost function is very irregular, this can actually help the\n",
    "algorithm jump out of local minima, so Stochastic Gradient Descent has a better\n",
    "chance of finding the global minimum than Batch Gradient Descent does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore randomness is good to escape from local optima, but bad because it means\n",
    "that the algorithm can never settle at the minimum. One solution to this dilemma is\n",
    "to gradually reduce the learning rate. The steps start out large (which helps make\n",
    "quick progress and escape local minima), then get smaller and smaller, allowing the\n",
    "algorithm to settle at the global minimum. This process is called simulated annealing,\n",
    "because it resembles the process of annealing in metallurgy where molten metal is\n",
    "slowly cooled down. The function that determines the learning rate at each iteration\n",
    "is called the learning schedule. If the learning rate is reduced too quickly, you may get\n",
    "stuck in a local minimum, or even end up frozen halfway to the minimum. If the\n",
    "learning rate is reduced too slowly, you may jump around the minimum for a long\n",
    "time and end up with a suboptimal solution if you halt training too early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 50 # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta = np.random.randn(2,1) # random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By convention we iterate by rounds of m iterations; each round is called an epoch.\n",
    "While the Batch Gradient Descent code iterated 1,000 times through the whole training\n",
    "set, this code goes through the training set only 50 times and reaches a fairly good\n",
    "solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.14994363],\n",
       "       [2.76637774]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform Linear Regression using SGD with Scikit-Learn, you can use the SGDRe\n",
    "gressor class, which defaults to optimizing the squared error cost function. The following\n",
    "code runs 50 epochs, starting with a learning rate of 0.1 (eta0=0.1), using the\n",
    "default learning schedule (different from the preceding one), and it does not use any\n",
    "regularization (penalty=None; more details on this shortly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Agus Richard Lubis\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "       eta0=0.1, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='invscaling', loss='squared_loss', max_iter=None,\n",
       "       n_iter=50, n_iter_no_change=5, penalty=None, power_t=0.25,\n",
       "       random_state=None, shuffle=True, tol=None, validation_fraction=0.1,\n",
       "       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg = SGDRegressor(n_iter=50, penalty=None, eta0=0.1)\n",
    "sgd_reg.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.1573087]), array([2.7047766]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last Gradient Descent algorithm we will look at is called Mini-batch Gradient Descent. It is quite simple to understand once you know Batch and Stochastic Gradient Descent: at each step, instead of computing the gradients based on the full training set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-batch GD computes the gradients on small random sets of instances called minibatches. The main advantage of Mini-batch GD over Stochastic GD is that you can get a performance boost from hardware optimization of matrix operations, especially when using GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm’s progress in parameter space is less erratic than with SGD, especially with fairly large mini-batches. As a result, Mini-batch GD will end up walking around a bit closer to the minimum than SGD. But, on the other hand, it may be harder for it to escape from local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normal Equation:\n",
    "    - Large m: Fast\n",
    "    - Out-of-core support: No\n",
    "    - Large n: Slow\n",
    "    - Hyperparameters: 0\n",
    "    - Scaling required: No\n",
    "    - Scikit-learn: LinearRegression\n",
    "- Batch GD:\n",
    "    - Large m: Slow\n",
    "    - Out-of-core support: No\n",
    "    - Large n: Fast\n",
    "    - Hyperparameters: 2\n",
    "    - Scaling required: Yes\n",
    "    - Scikit-learn: n/a\n",
    "- Stochastic GD:\n",
    "    - Large m: Fast\n",
    "    - Out-of-core support: Yes\n",
    "    - Large n: Fast\n",
    "    - Hyperparameters: >= 2 \n",
    "    - Scaling required: Yes\n",
    "    - Scikit-learn: SGDRegressor\n",
    "- Mini-batch GD:\n",
    "    - Large m: Fast\n",
    "    - Out-of-core support: Yes\n",
    "    - Large n: Fast\n",
    "    - Hyperparameters: >= 2 \n",
    "    - Scaling required: Yes\n",
    "    - Scikit-learn: n/a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
