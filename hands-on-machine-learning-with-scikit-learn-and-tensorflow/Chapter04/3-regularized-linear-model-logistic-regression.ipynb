{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce overfitting -> regularize the model\n",
    "Set constraints to the model so the model use less features/weaker features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression (also called Tikhonov regularization) is a regularized version of Linear Regression: a regularization term equal to $\\sum_{1=1}^n \\theta_{i}^{2}$ is added to the cost function. This forces the learning algorithm to not only fit the data but aslo keep the model weights as small as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter α controls how much you want to regularize the model. If α = 0 then Ridge Regression is just Linear Regression. If α is very large, then all weights end up very close to zero and the result is a flat line going through the data’s mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression cost function:\n",
    "\n",
    "$J(\\theta) = MSE(\\theta) + \\alpha\\frac{1}{2}\\sum_{i=1}^{n}\\theta^{2}_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the bias term $\\theta_{0}$ is not regularized. If we define w as the vector of feature weights (θ1 to θn), then the regularization term is simply equal to ½(∥ w ∥2)2, where ∥ · ∥2 represents the ℓ2 norm of the weight vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to scale the data (e.g., using a StandardScaler) before performing Ridge Regression, as it is sensitive to the scale of the input features. This is true of most regularized models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing $\\alpha$ leads to flatter (i.e., less extreme, more reasonable) predictions; this reduces the model’s variance but increases its bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression closed-form solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{\\theta} = (X^T.X + \\alpha A)^-1 . X^T . y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use matrix factorization technique by Andre-Louis Cholesky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg = Ridge(alpha=0, solver='cholesky').fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.10855808]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Agus Richard Lubis\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(penalty='l2').fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.03638107])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The penalty hyperparameter sets the type of regularization term to use. Specifying\n",
    "\"l2\" indicates that you want SGD to add a regularization term to the cost function\n",
    "equal to half the square of the ℓ2 norm of the weight vector: this is simply Ridge\n",
    "Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Least Absolute Shrinkage and Selection Operator Regression (simply called Lasso\n",
    "Regression) is another regularized version of Linear Regression: just like Ridge\n",
    "Regression, it adds a regularization term to the cost function, but it uses the ℓ1 norm\n",
    "of the weight vector instead of half the square of the ℓ2 norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso Regression cost function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(\\theta) = MSE(\\theta) + \\alpha \\sum_{i=1}^{n}  \\left|\\theta_{i}\\right|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important characteristic of Lasso Regression is that it tends to completely eliminatethe weights of the least important features (i.e., set them to zero). In other words, Lasso Regression automatically performs feature selection and outputs a sparse model (i.e., with few nonzero feature weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg = Lasso(alpha=0.1).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.06034714])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The\n",
    "regularization term is a simple mix of both Ridge and Lasso’s regularization terms,\n",
    "and you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge\n",
    "Regression, and when r = 1, it is equivalent to Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(\\theta) = MSE(\\theta) + r \\alpha \\sum_{i=1}^{n} |\\theta| + \\frac{1-r}{2} \\alpha \\sum_{i=1}^{n} \\theta_{i}^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when should you use Linear Regression, Ridge, Lasso, or Elastic Net? It is almost\n",
    "always preferable to have at least a little bit of regularization, so generally you should\n",
    "avoid plain Linear Regression. Ridge is a good default, but if you suspect that only a\n",
    "few features are actually useful, you should prefer Lasso or Elastic Net since they tend\n",
    "to reduce the useless features’ weights down to zero as we have discussed. In general,\n",
    "Elastic Net is preferred over Lasso since Lasso may behave erratically when the number\n",
    "of features is greater than the number of training instances or when several features\n",
    "are strongly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.06362226])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elastic_net.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very different way to regularize iterative learning algorithms such as Gradient Descent is to stop training as soon as the validation error reaches a minimum. This is called early stopping. As the epochs go by, the algorithm learns and its prediction error (RMSE) on the training set naturally goes down, and so does its prediction error on the validation set. However,after a while the validation error stops decreasing and actually starts to go back up. This indicates that the model has started to overfit the training data. With early stopping you just stop training as soon as the validation error reaches the minimum. It is such a simple and efficient regularization technique that Geoffrey Hinton called it a “beautiful free lunch.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Stochastic and Mini-batch Gradient Descent, the curves are not so smooth, and it may be hard to know whether you have reached the minimum or not. One solution is to stop only after the validation error has been above the minimum for some time (when you are confident that the model will not do any better), then roll back the model parameters to the point where the validation error was at a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('poly_features', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly_scaled = pipe.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_reg = SGDRegressor(n_iter=1, warm_start=True, penalty=None, \n",
    "                      learning_rate='constant', eta0=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum_val_error = float('inf')\n",
    "# best_epoch = None\n",
    "# best_model = None\n",
    "# for epoch in range(1000):\n",
    "#     sgd_reg.fit(X_train_poly_scaled, y_train) # continues where it left off\n",
    "#     y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
    "#     val_error = mean_squared_error(y_val_predict, y_val)\n",
    "#     if val_error < minimum_val_error:\n",
    "#         minimum_val_error = val_error\n",
    "#         best_epoch = epoch\n",
    "#         best_model = clone(sgd_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression (also called Logit Regression) is commonly used to estimate the probability that an instance belongs to a particular class (e.g., what is the probability that this email is spam?). If the estimated probability is greater than 50%, then the model predicts that the instance belongs to that class (called the positive class, labeled “1”), or else it predicts that it does not (i.e., it belongs to the negative class, labeled “0”). __This makes it a binary classifier.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how does it work? Just like a Linear Regression model, a Logistic Regression\n",
    "model computes a weighted sum of the input features (plus a bias term), but instead\n",
    "of outputting the result directly like the Linear Regression model does, it outputs the\n",
    "logistic of this result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression model estimated probability (vectorized form)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{p} = h_{\\theta}(x) = \\sigma(\\theta^T . x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma()$ represents sigmoid function, that outputs a number between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid function:\n",
    "\n",
    "$\\sigma(t) = \\frac{1}{1 + e^{-t}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instance x belongs to the positive class, it can make its prediction ŷ easily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\hat{y} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "    0 & \\hat{p} < 0.5,\\\\\n",
    "    1 & \\hat{p} \\geq 0.5. \\\\\n",
    "\\end{array}\n",
    "\\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, now you know how a Logistic Regression model estimates probabilities and\n",
    "makes predictions. But how is it trained? The objective of training is to set the parameter\n",
    "vector θ so that the model estimates high probabilities for positive instances (y = 1) and low probabilities for negative instances (y = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function of a single training instance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ c(\\theta) = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "    -log(\\hat{p}) & y = 1, \\\\\n",
    "    -log(1 - \\hat{p}) & y = 0. \\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cost function makes sense because – log(t) grows very large when t approaches 0, so the cost will be large if the model estimates a probability close to 0 for a positive instance, and it will also be very large if the model estimates a probability close to 1 for a negative instance. On the other hand, – log(t) is close to 0 when t is close to 1, so the cost will be close to 0 if the estimated probability is close to 0 for a negative instance or close to 1 for a positive instance, which is precisely what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression cost function (log loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)}log(\\hat{p}^{(i)}) + (1-y^{(i)})log(1-\\hat{p}^{(i)})]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bad news: there is no equivalent normal function to computes the value of $\\theta$ that minimizes the cost function. But the good news is that this cost function is convex, so Gradient Descent (or any other optimization algorithm) is guaranteed to find the global minimum (if the learning rate is not too large and you wait long enough). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic cost function partial derivatives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial}{\\partial\\theta_j} J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m (\\sigma(\\theta^{T} x^{(i)}) - y^{(i)})x^{(i)}_{j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equation looks very much like Equation 4-5: for each instance it computes the\n",
    "prediction error and multiplies it by the jth feature value, and then it computes the\n",
    "average over all training instances. Once you have the gradient vector containing all\n",
    "the partial derivatives you can use it in the Batch Gradient Descent algorithm. That’s\n",
    "it: you now know how to train a Logistic Regression model. For Stochastic GD you\n",
    "would of course just take one instance at a time, and for Mini-batch GD you would\n",
    "use a mini-batch at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(iris.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris['data'][:, 3:] # petal widht\n",
    "y = (iris['target'] == 2).astype(np.int) # 1 if iris-virginica, else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Agus Richard Lubis\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_new, y_proba[:, 1], 'g-', label='Iris-Virginica')\n",
    "plt.plot(X_new, y_proba[:, 0], 'b--', label='Not Iris-Virginica')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The petal width of Iris-Virginica flowers (represented by triangles) ranges from 1.4 cm to 2.5 cm, while the other iris flowers (represented by squares) generally have a smaller petal width, ranging from 0.1 cm to 1.8 cm. Notice that there is a bit of overlap. Above about 2 cm the classifier is highly confident that the flower is an Iris- Virginica (it outputs a high probability to that class), while below 1 cm it is highly confident that it is not an Iris-Virginica (high probability for the “Not Iris-Virginica” class). In between these extremes, the classifier is unsure. However, if you ask it to predict the class (using the predict() method rather than the predict_proba() method), it will return whichever class is the most likely. Therefore, there is a decision boundary at around 1.6 cm where both probabilities are equal to 50%: if the petal width is higher than 1.6 cm, the classifier will predict that the flower is an Iris- Virginica, or else it will predict that it is not (even if it is not very confident):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.predict([[1.7], [1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularization parameter for logistic regression is C, the higher the value of C, the less the model is regularized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the multinomial logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is quite simple: when given an instance x, the Softmax Regression model first computes a score sk(x) for each class k, then estimates the probability of each class by applying the softmax function (also called the normalized exponential) to the scores. The equation to compute sk(x) should look familiar, as it is just like the equation for Linear Regression prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax score for class k:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$s_k(x) = \\theta_{k}^{T} . x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each class has its own dedicated parameter vector $\\theta_k$. All these vectors are typically stored as rows in a parameter matrix $\\Theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have computed the score of every class for the instance x, you can estimate the probability pk that the instance belongs to class k by running the scores through the softmax function: it computes the exponential of every socre then normalizes them (dividing by the summ of all the exponentials)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{p}_k = \\sigma(s(x))_k = \\frac{exp(s_k(x))}{\\sum_{j=1}^{K} exp(s_j(x))}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- K is the number of classes\n",
    "- s(x) is a vector containing the scores of each class for the instance x\n",
    "- $\\theta(s(x))_k$ is the estimated probabilty that the instance x belongs to class k given the scores of each class for that instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax classifier predicts the clas with the highest estimated probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax Regression classifier prediction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{y} = argmax_k \\sigma(s(x))_k = argmax s_k(x) = argmax(\\theta_k^T . x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argmax operator returns the value of a variable that minimizes a function In this equation, it returns the value of k that maximizes the estimated probability σ(s(x))k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The Softmax Regression classifier predicts only one class at a time (i.e., it is multiclass, not multioutput) so it should be used only with mutually exclusive classes such as different types of plants. You cannot use it to recognize multiple people in one picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know how the model estimates probabilities and makes predictions, let’s take a look at training. The objective is to have a model that estimates a high probability for the target class (and consequently a low probability for the other classes). Minimizing the cost function called the cross entropy, should lead to this objective because it penalizes the model when it estimatesa low probability for a target class. Cross entropy is frequently used to measure how well a set of estimated class probabilities match the target classes (we will use it again several times in the following chapters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross entropy cost function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(\\Theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y_k^{(i)} log (\\hat{p}_k^{(i)})$\n",
    "\n",
    "$y_k^{(i)}$ is equal to 1 if the target class for the ith instance is k; otherwise, it is equal to0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross entropy gradient vector for class k:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_{\\theta_{k}} J(\\Theta) = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{p}_{k}^{(i)} - y_k^{(i)}) x^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn’s LogisticRegression uses one-versus-all by default when you train it on more than two classes, but you can set the multi_class hyperparameter to \"multinomial\" to switch it to Softmax Regression instead. You must also specify a solver that supports Softmax Regression, such as the \"lbfgs\" solver. It also applies ℓ2 regularization by default, which you can\n",
    "control using the hyperparameter C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris['data'][:, (2, 3)] # petal length, petal width\n",
    "y = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', C=10)\n",
    "softmax_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_reg.predict([[5, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.38014896e-07, 5.74929995e-02, 9.42506362e-01]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_reg.predict_proba([[5, 2]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
